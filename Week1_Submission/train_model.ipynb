{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517bdd1e-105f-4fa5-a969-d71c4b80290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.applications import EfficientNetV2B2\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import display\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d22df9-6989-4311-b271-5cedf529cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paths\n",
    "base_path = 'TrashType_Image_Dataset'  # Folder containing subfolders of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b66d88-13ee-44c1-bc0d-ad32d2f51500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2024 images belonging to 6 classes.\n",
      "Found 503 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    base_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "val_gen = train_datagen.flow_from_directory(\n",
    "    base_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3a224f-e090-4d7a-80f1-1ec5f38a7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model with EfficientNetV2B2\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "base_model = EfficientNetV2B2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_tensor=Input(shape=input_shape)\n",
    ")\n",
    "base_model.trainable = False  # freeze for now\n",
    "\n",
    "# Add custom layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c861f9d-8aa7-4dad-a293-72ae89371203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anura\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.6153 - loss: 1.0570 - val_accuracy: 0.8310 - val_loss: 0.4802\n",
      "Epoch 2/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.8734 - loss: 0.4048 - val_accuracy: 0.8569 - val_loss: 0.4417\n",
      "Epoch 3/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 972ms/step - accuracy: 0.8824 - loss: 0.3354 - val_accuracy: 0.8767 - val_loss: 0.4074\n",
      "Epoch 4/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.9171 - loss: 0.2482 - val_accuracy: 0.8728 - val_loss: 0.4229\n",
      "Epoch 5/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - accuracy: 0.9235 - loss: 0.2444 - val_accuracy: 0.8787 - val_loss: 0.4150\n",
      "Epoch 6/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 954ms/step - accuracy: 0.9382 - loss: 0.1980 - val_accuracy: 0.8767 - val_loss: 0.4269\n",
      "Epoch 7/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 958ms/step - accuracy: 0.9554 - loss: 0.1744 - val_accuracy: 0.8827 - val_loss: 0.4377\n",
      "Epoch 8/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 960ms/step - accuracy: 0.9524 - loss: 0.1506 - val_accuracy: 0.8807 - val_loss: 0.4356\n",
      "Epoch 9/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 999ms/step - accuracy: 0.9508 - loss: 0.1538 - val_accuracy: 0.8807 - val_loss: 0.4493\n",
      "Epoch 10/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 964ms/step - accuracy: 0.9509 - loss: 0.1448 - val_accuracy: 0.8867 - val_loss: 0.4503\n",
      "Epoch 11/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 950ms/step - accuracy: 0.9609 - loss: 0.1192 - val_accuracy: 0.8986 - val_loss: 0.4654\n",
      "Epoch 12/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 945ms/step - accuracy: 0.9620 - loss: 0.1181 - val_accuracy: 0.8807 - val_loss: 0.4496\n",
      "Epoch 13/13\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 919ms/step - accuracy: 0.9670 - loss: 0.0925 - val_accuracy: 0.8827 - val_loss: 0.4926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22259ac0560>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_gen, epochs=13, validation_data=val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a296a2-d5ee-4441-9182-5a5f8119566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model trained and saved as garbage_classifier_effnet.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save(\"garbage_classifier_effnet.h5\")\n",
    "print(\"\\nâœ… Model trained and saved as garbage_classifier_effnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f23359e-79ed-4dc1-93e9-1100eb10aa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model.save(\"efficientnet_garbage_classifier.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488f5d75-a534-4395-a48f-f20439372e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels (same order as training)\n",
    "class_labels = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b471fa-aafc-490e-b4d6-bd0eebabac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "def predict_material(img):\n",
    "    try:\n",
    "        print(\"Image received\")\n",
    "        img = img.resize((224, 224))\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        print(\"Preprocessing done\")\n",
    "\n",
    "        prediction = model.predict(img_array)\n",
    "        print(\"Prediction done\")\n",
    "\n",
    "        predicted_class = class_labels[np.argmax(prediction)]\n",
    "        confidence = np.max(prediction) * 100\n",
    "\n",
    "        return f\"ğŸ“¦ Material: {predicted_class}\\nğŸ¯ Accuracy: {confidence:.2f}%\"\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return f\"âŒ Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86320f9-03af-47e7-824c-e77371c9fced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Gradio UI\n",
    "import gradio as gr\n",
    "interface = gr.Interface(\n",
    "    fn=predict_material,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"â™»ï¸ Garbage Classification\",\n",
    "    description=\"Upload an image of garbage (plastic, paper, metal, etc.) to classify its type.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d3385c-9e9b-46c0-b881-920346107623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cef8b-83cf-4a82-a1aa-b0e26aae5597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
